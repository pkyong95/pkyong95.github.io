<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.7.0">

  <meta name="author" content="Kwanyong Park">
  <meta name="description" content="Research Scientist">
  <link rel="alternate" hreflang="en-us" href="/">
  <meta name="theme-color" content="#2962ff">
   
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous"> 
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
  
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  <link rel="stylesheet" href="/css/academic.css">
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Kwanyong Park">
  <link rel="manifest" href="/index.webmanifest">
  <!-- <link rel="icon" type="image/png" href="/images/icon_hu2065e2b3e3f9ec61858579cdb7515c4d_74858_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu2065e2b3e3f9ec61858579cdb7515c4d_74858_192x192_fill_lanczos_center_2.png"> -->

  <link rel="canonical" href="/">
  <meta property="twitter:card" content="summary">
  <meta property="og:site_name" content="Kwanyong Park">
  <meta property="og:url" content="/">
  <meta property="og:title" content="Kwanyong Park">
  <meta property="og:description" content="Research Scientist"><meta property="og:image" content="img/map[gravatar:%!s(bool=false) shape:circle]">
  <meta property="twitter:image" content="img/map[gravatar:%!s(bool=false) shape:circle]"><meta property="og:locale" content="en-us">
  

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "WebSite",
  "potentialAction": {
    "@type": "SearchAction",
    "target": "/?q={search_term_string}",
    "query-input": "required name=search_term_string"
  },
  "url": "/"
}
</script>
  <title>Kwanyong Park</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#navbar-main" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
      </div>

    </section>
  </div>
</aside>


<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Kwanyong Park</a>
    </div>
    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>

    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Kwanyong Park</a>
    </div>

    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content"> 
      <ul class="navbar-nav d-md-inline-flex">
        <li class="nav-item">
          <a class="nav-link " href="/#about" data-target="#about"><span>Home</span></a>
        </li>

        <li class="nav-item">
          <a class="nav-link " href="/#papers" data-target="#papers"><span>Publications</span></a>
        </li>

        <li class="nav-item">
          <a class="nav-link " href="/#awards" data-target="#awards"><span>Awards &amp; Honors</span></a>
        </li>


        <li class="nav-item">
          <a class="nav-link " href="/#activities" data-target="#activities"><span>Activities</span></a>
        </li>


      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>

    </ul>

  </div>
</nav>


<span class="js-widget-page d-none"></span>

  <section id="about" class="home-section wg-about   " style="padding: 30px 0 20px 0;" >
    <div class="container">

<div class="row">
  <div class="col-12 col-lg-4">
    <div id="profile">
      <img class="avatar avatar-circle" src="/authors/admin/profile_initial.jpg" alt="Avatar">
      <div class="portrait-title">
        <h2>Kwanyong Park</h2>
        <h3>Assistant Professor</h3>
        
        <h3>
          <a href="https://www.uos.ac.kr/en/main.do" target="_blank" rel="noopener">
          <span>University of Seoul</span>
          </a>
        </h3>
        <h3>
          <a href="https://visagi.uos.ac.kr/" target="_blank" rel="noopener">
          <span>VisAGI Lab@UOS</span>
          </a>
        </h3>
      </div>

      
      <td>
      <br>
      <a href="cv/Kwanyong_cv_2025_03_05.pdf"><b>CV</b></a>
        | <a href="https://scholar.google.com/citations?user=5EwTX0YAAAAJ"><b>Google Scholar</b></a>
        | <a href="https://github.com/pkyong95"><b>Github</b></a>
      <br>
      <br>
      </td>

    </div>
  </div>
  <div class="col-12 col-lg-8">

    
<br>
<p>I am an Assistant Professor in the <a href="https://engineering.uos.ac.kr/engineering/depart/cs/main.do" target="_blank" rel="noopener">Department of Computer Science and Engineering</a> at <a href="https://www.uos.ac.kr/en/main.do" target="_blank" rel="noopener">University of Seoul</a>, where I lead the <a href="https://visagi.uos.ac.kr/" target="_blank" rel="noopener">Visual and General Intelligence Lab</a>.
  
<p>Previously, I was a Researcher at <a href="https://www.etri.re.kr/eng/main/main.etri" target="_blank" rel="noopener">Electronics and Telecommunications Research Institute (ETRI)</a>.
  I received Ph.D. and M.S. degrees in <a href="https://ee.kaist.ac.kr/" target="_blank" rel="noopener">Electrical Engineering</a> 
  from the <a href="https://kaist.ac.kr/" target="_blank" rel="noopener">Korea Advanced Institute of Science and Technology (KAIST)</a>, 
  where I was advised by <a href="http://rcv.kaist.ac.kr/" target="_blank" rel="noopener">In So Kweon</a>. 
  During my Ph.D. studies, I interned at <a href="https://research.adobe.com/" target="_blank" rel="noopener">Adobe Research</a> and was honored with the <a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/winners" target="_blank" rel="noopener">Qualcomm Innovation Fellowship</a>.</p>

<p> My research aims to build robust multi-modal AI models capable of understanding and generating complex real-world scenarios, with a specific focus on co-designing effective data and learning frameworks. 
My primary research interests include the following areas, but also open to exploring other challenging and impactful problems. 
<ul>
<!-- <li>Video Understanding and Processing</li> 
<li>Simulated Learning and Domain Adaptation</li> 
<li>Unsupervised and Self-supervised Learining</li> -->
<!-- <li>Data-efficient Learning</li> 
<li>Large-scale Dataset Construction</li> 
<li>Video Understanding and Processing</li> 
<li>Multi-modal Learning</li>
<li>Generative Models</li> -->

<!-- <li>Scalable and Efficient Learning; Data-efficient Learning, Multi-modal Learning</li>
<li>Generative AI; Image/Video Generation, Multimodal Large Language Models</li>
<li>Visual Recognition; Video Understanding and Processing</li>  -->

<li>Scalable and Efficient Learning
<br>- Multimodal Learning & Data-efficient Learning<br>
</li>
<li>Data-centric AI
<br>- Effective Large-scale Dataset Collection, Generation, and Curation<br>
</li>
<li>Generative AI
<br>- Image/Video Generation & Multi-modal Large Language Models<br>
</li>

  
</ul>
<!-- but not limited to. -->
</p>

    <div class="row">

      <div class="col-md-6">
        <h3>Contact</h3>
        <ul class="ul-contact fa-ul">
          <li>          
            <i class="fa-li fas fa-envelope"></i>
            <div class="description">
              <p class="course">kwanyong.park [at] uos.ac.kr</p>
              <p class="course">pkyong7 [at] kaist.ac.kr</p>
<!--               <p class="course">pkyong7 [at] gmail.com</p> -->
            </div>
          </li>
          <li>  
            <i class="fa-li fas fa-map-marker"></i>
            <div class="description">
              <p class="course">163 Seoulsiripdaero, Dongdaemun-gu, Seoul 02504, Republic of Korea</p>
            </div> 
          </li>
          <!-- <li>  
            <i class="fa-li fas fa-phone-alt"></i>
            <div class="description">
              <p class="course">(&#43;82)-42-350-3579</p>
            </div> 
          </li>  -->         
        </ul>        
      </div>

      <div class="col-md-6">
        <h3>Education</h3>
        <ul class="ul-edu fa-ul">
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">PhD, Major in EE, KAIST, 2023</p>
<!--               <p class="institution">KAIST, Korea</p> -->
              <p class="institution">on "Towards Universal Visual Scene Understanding in the Wild"</p>
              <p class="institution">Advisor: Prof. In So Kweon</p>
            </div>
          </li>
          
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">MS, Major in EE, KAIST, 2019</p>
<!--               <p class="institution">KAIST, Korea</p> -->
              <p class="institution">on "Learning unpaired video-to-video translation for domain adaptation"</p>
              <p class="institution">Advisor: Prof. In So Kweon</p>
            </div>
          </li>
          
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">BS, Double Major in ME and EE, KAIST, 2018</p>
<!--               <p class="institution">KAIST, Korea</p> -->
            </div>
          </li>
        </ul>
      </div>
    </div>
  </div>
</div>

    </div>
  </section>



<section id="experience" class="home-section wg-blank   " style="padding: 10px 0 10px 0;" >
<div class="container">

<div class="row">
    <div class="col-lg-12">
    <h1>Research Experiences</h1>
      <!-- <h3 id="experience">Research Experiences</h3> -->
<ul>
  
<li> <div style="float:left"><b>University of Seoul</b>, Seoul, Korea</div> <div style="float:right">Mar 2025 - Present</div> <br>
    Assistant Professor, Department of Computer Science and Engineering <br>
    Principal Investigator, <a href="https://visagi.uos.ac.kr/">Visual and General Intelligence (VisAGI) Lab</a>
</li>
  
<li> <div style="float:left"><b>ETRI</b>, Daejeon, Korea (Military Service)</div> <div style="float:right">Sep 2023 - Feb 2025</div> <br>
    Researcher, <a href="https://etri-visualintelligence.github.io/" target="_blank" rel="noopener">Visual Intelligence Lab</a> <br>
</li>
  
<li> <div style="float:left"><b>Adobe Research</b>, San Jose, CA (Remote)</div> <div style="float:right">Apr 2021 - Dec 2021</div> <br>
    Research Intern, Deep Learning Group, Creative Intelligence Lab <br>
    Mentor : <a href="https://joonyoung-cv.github.io/">Joon-Young Lee</a>, <a href="https://sites.google.com/view/seoungwugoh">SeoungWug Oh</a>
</li>

<li> <div style="float:left"><b>KAIST</b>, Daejeon, Korea</div> <div style="float:right">Mar 2018 - Aug 2023</div> <br>
    Graduate Student Researcher, <a href="https://rcv.kaist.ac.kr/">Robotics and Computer Vision Lab</a>
</li>

</ul>
    </div>
</div>
    </div>
  </section>

  <section id="papers" class="home-section wg-papers   " style="padding: 20px 0 20px 0;" >
    <div class="container">
      
<div class="row">
  
    <div class="col-lg-12">
      <h1>Publications</h1>
    </div>
    <div class="row">
        <ul class="ul-papers">

        <li>
            <div class="img">
              <img src="/papers/images/blank.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">Learning Compositionality from Multifaceted Synthetic Data for Language-based Object Detection</p>
            <p class="authors"><b>Kwanyong Park</b>, Sojung An, Yong Jae Lee, Donghyun Kim</p>
            <p class="venue">IJCV 2025<br>
            <p class="resources">
              [
                Paper
              ]
            </p>
          </div>
        </li>
          
        <li>
            <div class="img">
              <img src="/papers/images/iccv25_captionsmith.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">CaptionSmiths: Flexibly Controlling Language Pattern in Image Captioning</p>
            <p class="authors">Kuniaki Saito, Donghyun Kim, <b>Kwanyong Park</b>, Atsushi Hashimoto, Yoshitaka Ushiku</p>
            <p class="venue">ICCV 2025 [<span style="color:red">Highlight</span>] <span style="font-weight:normal"></span></p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2507.01409">Paper</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/blank.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">Hierarchical Entailment Representations for Linguistic Compositionality in Language-based Object Detection</p>
            <p class="authors">Sojung An, <b>Kwanyong Park</b>, Yong Jae Lee, Donghyun Kim</p>
            <p class="venue">ICCV 2025 Workshop on "What is Next in Multimodal Foundation Models?"<br>
            <p class="resources">
              [
                Paper
              ]
            </p>
          </div>
        </li>
          
        <li>
            <div class="img">
              <img src="/papers/images/holisafe.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">HoliSafe: Holistic Safety Benchmarking and Modeling with Safety Meta Token for Vision-Language Model</p>
            <p class="authors">Youngwan Lee, Kangsan Kim, <b>Kwanyong Park</b>, Ilchae Jung, Sujin Jang, Seanie Lee, Yong-Ju Lee, Sung Ju Hwang</p>
            <p class="venue">arXiv<br>
            <p class="resources">
              [
                <a href="https://www.arxiv.org/abs/2506.04704">Paper</a> /
                <a href="https://youngwanlee.github.io/holisafe/">Project page</a>
              ]
            </p>
          </div>
        </li>
          

        <li>
            <div class="img">
              <img src="/papers/images/neuripsw24.PNG" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">A Multimodal Chain of Tools for Described Object Detection</p>
            <p class="authors"><b>Kwanyong Park</b>, Youngwan Lee, Yong-Ju Lee</p>
            <p class="venue">NeurIPS 2024 Workshop on "Compositional Learning"<br>
            <p class="resources">
              [
                <a href="https://openreview.net/pdf?id=N4i4PfcrK6">Paper</a>
              ]
            </p>
          </div>
        </li>
          
        <li>
            <div class="img">
              <img src="/papers/images/koala_v3.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">KOALA: Empirical Lessons Toward Memory-Efficient and Fast Diffusion Models for Text-to-Image Synthesis</p>
            <p class="authors">Youngwan Lee, <b>Kwanyong Park</b>, Yoorhim Cho, Yong-Ju Lee, Sung Ju Hwang</p>
            <p class="venue">NeurIPS 2024<br>
            <p class="venue"><span style="font-weight:normal"><em>*Also presented at</em></span> CVPR 2024 Workshop on "Generative Models for Computer Vision"</p>
            <p class="other"> <i><span style="font-weight:bolder;color:#BB2222">Media coverage: covered by <a href="https://www.ytn.co.kr/_ln/0106_202402040423528132">YTN,</a> <a href="https://www.yna.co.kr/view/AKR20240126075300063">Yonhap News,</a> <a href="https://www.aitimes.kr/news/articleView.html?idxno=30153">AI Times,</a> and many local media</span></i></p>
            </p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2312.04005">Paper</a> /
                <a href="https://youngwanlee.github.io/KOALA/">Project page</a> /
                <a href="https://github.com/youngwanLEE/sdxl-koala">Code</a>
              ]
            </p>
          </div>
        </li>
          
        <li>
            <div class="img">
              <img src="/papers/images/eccv24_wscl.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">Weak-to-Strong Compositional Learning from Generative Models for Language-based Object Detection</p>
            <p class="authors"><b>Kwanyong Park</b>, Kuniaki Saito, Donghyun Kim</p>
            <p class="venue">ECCV 2024<br>
            <p class="venue"><span style="font-weight:normal"><em>*Also presented at</em></span> CVPR 2024 Workshop on "Generative Models for Computer Vision"</p>
            <p class="other"> <i><span style="font-weight:bolder;color:#BB2222">3rd place in the OmniLabel Challenge @ ECCV2024</span></i></p>
            </p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2407.15296">Paper</a>
              ]
            </p>
          </div>
        </li>
          
        <li>
            <div class="img">
              <img src="/papers/images/cvpr24_mtmmc.gif" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">MTMMC: A Large-Scale Real-World Multi-Modal Camera Tracking Benchmark</p>
            <p class="authors">Sanghyun Woo*, <b>Kwanyong Park*</b>, Inkyu Shin*, Myungchul Kim*, In So Kweon</p>
            <p class="venue">CVPR 2024<br>
            </p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2403.20225">Paper</a> /
                <a href="https://sites.google.com/view/mtmmc/home">Project page</a>
              ]
            </p>
          </div>
        </li>
          
          
        <li>
            <div class="img">
              <img src="/papers/images/RAL24_ccdtta.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">Test-time Adaptation in the Dynamic World with Compound Domain Knowledge Management</p>
            <p class="authors">Junha Song, <b>Kwanyong Park</b>, Inkyu Shin, Sanghyun Woo, Chaoning Zhang, and In So Kweon</p>
            <p class="venue">RAL-ICRA 2024 <br>
            </p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2212.08356">Paper</a>
              ]
            </p>
          </div>
        </li>  
        
        <li>
            <div class="img">
              <img src="/papers/images/MVA23_Thermal.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">Joint Self-supervised Learning and Adversarial Adaptation for Monocular Depth Depth Estimation from Thermal Image</p>
            <p class="authors">Ukcheol Shin, <b>Kwanyong Park</b>, Byeong-Uk Lee, Kyunghyun Lee, In So Kweon</p>
            <p class="venue">MVA 2023 <br>
            </p>
            <p class="resources">
              [
                <a href="https://link.springer.com/article/10.1007/s00138-023-01404-3">Paper</a>
              ]
            </p>
          </div>
        </li>  
          
        <li>
            <div class="img">
              <img src="/papers/images/CVPR23_matting.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">Mask-guided Matting in the Wild</p>
            <p class="authors"><b>Kwanyong Park</b>, Sanghyun Woo, Seoung Wug Oh, In So Kweon, Joon-Young Lee</p>
            <p class="venue">CVPR 2023<br>
            </p>
            <p class="resources">
              [
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Park_Mask-Guided_Matting_in_the_Wild_CVPR_2023_paper.pdf">Paper</a>
              ]
            </p>
          </div>
        </li>  
          
          
        <li>
            <div class="img">
              <img src="/papers/images/aaai23_da.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">Bidirectional Domain Mixup for Domain Adaptive Semantic Segmentation</p>
            <p class="authors">Daehan Kim*, Minseok Seo*, <b>Kwanyong Park</b>, Inkyu Shin,  Sanghyun Woo, In So Kweon, Dong-Geol Choi<br/></p>
            <p class="venue">AAAI 2023</p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2303.09779">Paper</a>
              ]
            </p>
          </div>
        </li>
        
        <li>
            <div class="img">
              <img src="/papers/images/wacv23_unida.PNG" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">Learning Classifiers of Prototypes and Reciprocal Points for Universal Domain Adaptation</p>
            <p class="authors">Sungsu Hur, Inkyu Shin, <b>Kwanyong Park</b>, Sanghyun Woo, In So Kweon<br/></p>
            <p class="venue">WACV 2023</p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2212.08355">Paper</a>
              ]
            </p>
          </div>
        </li>
          
        <li>
            <div class="img">
              <img src="/papers/images/wacv23_thermal.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">Self-supervised Monocular Depth Estimation from Thermal Images via Adversarial Multi-spectral Adaptation</p>
            <p class="authors">Ukcheol Shin, <b>Kwanyong Park</b>, Byeong-Uk Lee, Kyunghyun Lee, In So Kweon<br/></p>
            <p class="venue">WACV 2023</p>
            <p class="other"> <i><span style="font-weight:bolder;color:#BB2222">Received Best Student Paper Award in WACV 2023</span></i></p>
            <p class="resources">
              [
                <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Shin_Self-Supervised_Monocular_Depth_Estimation_From_Thermal_Images_via_Adversarial_Multi-Spectral_WACV_2023_paper.pdf">Paper</a>
              ]
            </p>
          </div>
        </li>
          
        <li>
            <div class="img">
              <img src="/papers/images/eccv22_tao.jpg" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection</p>
            <p class="authors">Sanghyun Woo, <b>Kwanyong Park</b>, Seoung Wug Oh, In So Kweon, Joon-Young Lee<br/></p>
            <p class="venue">ECCV 2022</p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2212.10147">Paper</a>
              ]
            </p>
          </div>
        </li>
          
          
        <li>
            <div class="img">
              <img src="/papers/images/eccv22_clip.jpg" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">Tracking by Associating Clips</p>
            <p class="authors">Sanghyun Woo, <b>Kwanyong Park</b>, Seoung Wug Oh, In So Kweon, Joon-Young Lee<br/></p>
            <p class="venue">ECCV 2022</p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2212.10149">Paper</a>
              ]
            </p>
          </div>
        </li>
          
          
        <li>
            <div class="img">
              <img src="/papers/images/cvpr22_pcvos.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">Per-Clip Video Object Segmentation</p>
            <p class="authors"><b>Kwanyong Park</b>, Sanghyun Woo, Seoung Wug Oh, In So Kweon, Joon-Young Lee</p>
            <p class="venue">CVPR 2022 <br>
            </p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2208.01924">Paper</a> /
                <a href="https://github.com/pkyong95/PCVOS">Code</a>
              ]
            </p>
          </div>
        </li>
          
          
        <li>
            <div class="img">
              <img src="/papers/images/video_da.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">Unsupervised Domain Adaptation for Video Semantic Segmentation</p>
            <p class="authors">Inkyu Shin*, <b>Kwanyong Park*</b>, Sanghyun Woo, In So Kweon (*: equal contribution)</p>
            <p class="venue">arXiv<span style="font-weight:normal"></span></p>
            <p class="resources">
              [
                <a href="https://arxiv.org/pdf/2107.11052.pdf">Paper</a>
              ]          
            </p>
          </div>
        </li>  
          
          
        <li>
            <div class="img">
              <img src="/papers/images/LabOR_iccv_2021.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">LabOR: Labeling Only if Required for Domain Adaptive Semantic Segmentation</p>
            <p class="authors">Inkyu Shin, Dong-Jin Kim, Jae Won Cho, Sanghyun Woo, <b>Kwanyong Park</b>, In So Kweon</p>
            <p class="venue">ICCV 2021 [<span style="color:red">Oral</span>] <span style="font-weight:normal"></span></p>
            <p class="other"> <i><span style="font-weight:bolder;color:#BB2222">Received Qualcomm Innovation Award 2021</span></i></p>
            <p class="resources">
              [
                <a href="https://arxiv.org/pdf/2108.05570.pdf">Paper</a>
              ] 
            </p>
          </div>
        </li>  
          
          
        <li>
            <div class="img">
              <img src="/papers/images/nips_20.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">Discover, Hallucinate, and Adapt: Open Compound Domain Adaptation for Semantic Segmentation</p>
            <p class="authors"><b>Kwanyong Park</b>, Sanghyun Woo, Inkyu Shin, In So Kweon</p>
            <p class="venue">NeurIPS 2020 <span style="font-weight:normal"></span></p>
            <p class="other"> <i><span style="font-weight:bolder;color:#BB2222">Received Qualcomm Innovation Award 2021</span></i></p>
            <p class="resources">
              [
                <a href="https://proceedings.neurips.cc/paper/2020/file/7a9a322cbe0d06a98667fdc5160dc6f8-Paper.pdf">Paper</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2020_bmvc.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">Align-and-Attend Network for Globally and Locally Coherent Video Inpainting</p>
            <p class="authors">Sanghyun Woo, Dahun Kim, <b>Kwanyong Park</b>, Joon-Young Lee, In So Kweon</p>
            <p class="venue">BMVC 2020 <span style="font-weight:normal"></span></p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/1905.13066">Paper</a>
              ]
            </p>
          </div>
        </li>


        <li>
            <div class="img">
              <img src="/papers/images/2019_mm_domain.JPG" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">Preserving Semantic and Temporal Consistency for Unpaired Video-to-Video Translation</p>
            <p class="authors"><b>Kwanyong Park</b>, Sanghyun Woo, Dahun Kim, Donghyeon Cho, In So Kweon</p>
            <p class="venue">MM 2019 <span style="font-weight:normal"></span></p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/1908.07683"> Paper </a>
              ]
            </p>
          </div>
        </li>
     
        </ul>      
    </div>  
</div>
    </div>
  </section>


<section id="awards" class="home-section wg-blank   " style="padding: 20px 0 20px 0;" >
    <div class="container">

<div class="row">  
    <div class="col-lg-12">
      <h1>Awards &amp; Honors</h1>      
      <!-- <h2 id="ms-students">MS students</h2> -->
<ul>
<li>3rd place in the OmniLabel Challenge @ ECCV2024</li>
<li>WACV Best Student Paper Awards, Jan 2023</li>
<li>Qualcomm Innovation Fellowship ($4,000), Nov 2021</li>
<li>SIGMM Student Travel Grants, ACM SIGMM ($1,500), Nov 2019</li>
<li>Best M.S students, Eun Chong-Kwan Scholarship, KAIST ($2,000), Mar 2018</li>
<li>Magna Cum Laude, Graduation with honors, KAIST, Feb 2018</li>
<li>KAIST Challenge Awards, KAIST, May 2016</li>

  
</ul>
    </div>
</div>
    </div>
  </section>



<section id="activities" class="home-section wg-blank   " style="padding: 20px 0 20px 0;" >
    <div class="container">

<div class="row">  
    <div class="col-lg-12">
      <h1>Academic Activities</h1>      
      <h2 id="reviewer">Reviewer</h2>
<ul>
<li>Conference on Computer Vision and Pattern Recognition (CVPR): 2022~</li>
<li>International Conference on Computer Vision (ICCV): 2023~</li>
<li>European Conference on Computer Vision (ECCV): 2022~</li>
<li>Conference on Neural Information Processing Systems (NeurIPS): 2024~</li>
<li>International Conference on Learning Representations (ICLR): 2025~</li>
<li>International Conference on Machine Learning (ICML): 2025~</li>
<li>Transactions on Pattern Analysis and Machine Intelligence (TPAMI): 2022~</li>
<li>Transactions on Multimedia (TMM): 2023~</li>
<li>Association for the Advancement of Artificial Intelligence (AAAI): 2023~</li>
<li>British Machine Vision Conference (BMVC): 2020~</li>
<!-- <li>ECCV 2020</li>
<li>CVPR 2020</li>
<li>AAAI 2020, 2021</li>
<li>ICCV 2019</li>
<li>NeurIPS 2020</li>
<li>ICLR 2021</li>
<li>Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</li>
<li>Transactions on Neural Networks and Learning Systems (TNNLS)</li>
<li>Transactions on Image Processing (TIP)</li> -->

</ul>
      <h2 id="reviewer">Invited Talks</h2>
<ul>
<li>The Korean Institute of Broadcast and Media Engineers, June 2025</li>  
<li>Yonsei University, Mar 2025</li>  
<li>Institute of Embedded Engineering of Korea, Nov 2024</li>
<li>Korea Artificial Intelligence Conference, Sep 2024</li>
</ul>
  
    </div>
</div>
    </div>
  </section>
  
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script> 
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    <script>const code_highlighting = false;</script>
    <script>const isSiteThemeDark = false;</script>
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>

    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>

    <script src="/js/academic.min.a8d7005002cb4a052fd6d721e83df9ba.js"></script>


  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    
    .
    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
